{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_292560/1118751559.py:19: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# system level\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "\n",
    "# deep learning\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models,transforms\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# data \n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# local\n",
    "from nerf_qa.DISTS_pytorch.DISTS_pt import DISTS, prepare_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/ccl/Datasets/NeRF-QA\"\n",
    "REF_DIR = path.join(DATA_DIR, \"Reference\")\n",
    "SYN_DIR = path.join(DATA_DIR, \"NeRF-QA_videos\")\n",
    "SCORE_FILE = path.join(DATA_DIR, \"NeRF_VQA_MOS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, train_df):\n",
    "        super(VQAModel, self).__init__()\n",
    "        # Reshape data (scikit-learn expects X to be a 2D array)\n",
    "        X = train_df['DISTS'].values.reshape(-1, 1)  # Predictor\n",
    "        y = train_df['MOS'].values  # Response\n",
    "\n",
    "        # Create a linear regression model\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Print the coefficients\n",
    "        print(f\"Coefficient: {model.coef_[0]}\")\n",
    "        print(f\"Intercept: {model.intercept_}\")\n",
    "        self.dists_model = DISTS()\n",
    "        self.dists_weight = nn.Parameter(torch.tensor([model.coef_[0]], dtype=torch.float32))\n",
    "        self.dists_bias = nn.Parameter(torch.tensor([model.intercept_], dtype=torch.float32))\n",
    "\n",
    "    def compute_dists_with_batches(self, dataloader):\n",
    "        all_scores = []  # Collect scores from all batches as tensors\n",
    "\n",
    "        for dist_batch, ref_batch in dataloader:\n",
    "            ref_images = ref_batch.to(device)  # Assuming ref_batch[0] is the tensor of images\n",
    "            dist_images = dist_batch.to(device)  # Assuming dist_batch[0] is the tensor of images\n",
    "            scores = self.dists_model(ref_images, dist_images, require_grad=True, batch_average=False)  # Returns a tensor of scores\n",
    "            \n",
    "            # Collect scores tensors\n",
    "            all_scores.append(scores)\n",
    "\n",
    "        # Concatenate all score tensors into a single tensor\n",
    "        all_scores_tensor = torch.cat(all_scores, dim=0)\n",
    "\n",
    "        # Compute the average score across all batches\n",
    "        average_score = torch.mean(all_scores_tensor) if all_scores_tensor.numel() > 0 else torch.tensor(0.0).to(device)\n",
    "\n",
    "        return average_score\n",
    "        \n",
    "    def forward(self, dataloader):\n",
    "        raw_scores = self.compute_dists_with_batches(dataloader)\n",
    "        \n",
    "        # Normalize raw scores using the trainable mean and std\n",
    "        normalized_scores = raw_scores * self.dists_weight + self.dists_bias\n",
    "        return normalized_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m API key must be 40 characters long, yours was 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ccl/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ccl/Code/nerf-qa/wandb/run-20240217_184300-pnz3tywu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aizu-nerf/nerf-qa/runs/pnz3tywu' target=\"_blank\">glittering-orchid-25</a></strong> to <a href='https://wandb.ai/aizu-nerf/nerf-qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aizu-nerf/nerf-qa' target=\"_blank\">https://wandb.ai/aizu-nerf/nerf-qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aizu-nerf/nerf-qa/runs/pnz3tywu' target=\"_blank\">https://wandb.ai/aizu-nerf/nerf-qa/runs/pnz3tywu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/4\n",
      "Validation Refrences: ['ship_reference.mp4' 'truck_reference.mp4']\n",
      "Coefficient: -14.048971112634561\n",
      "Intercept: 4.465621463615414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/36 [00:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.78 GiB. GPU 0 has a total capacity of 11.76 GiB of which 918.19 MiB is free. Including non-PyTorch memory, this process has 10.47 GiB memory in use. Of the allocated memory 7.01 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/ccl/Code/nerf-qa/play.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m dataloader \u001b[39m=\u001b[39m create_dataloader(row, config\u001b[39m.\u001b[39mforward_batch_size)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39m# Compute score\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m predicted_score \u001b[39m=\u001b[39m model(dataloader)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m target_score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(row[\u001b[39m'\u001b[39m\u001b[39mMOS\u001b[39m\u001b[39m'\u001b[39m], device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m \u001b[39m# Compute loss\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/ccl/Code/nerf-qa/play.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, dataloader):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     raw_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_dists_with_batches(dataloader)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# Normalize raw scores using the trainable mean and std\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     normalized_scores \u001b[39m=\u001b[39m raw_scores \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdists_weight \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdists_bias\n",
      "\u001b[1;32m/home/ccl/Code/nerf-qa/play.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m ref_images \u001b[39m=\u001b[39m ref_batch\u001b[39m.\u001b[39mto(device)  \u001b[39m# Assuming ref_batch[0] is the tensor of images\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m dist_images \u001b[39m=\u001b[39m dist_batch\u001b[39m.\u001b[39mto(device)  \u001b[39m# Assuming dist_batch[0] is the tensor of images\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdists_model(ref_images, dist_images, require_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, batch_average\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)  \u001b[39m# Returns a tensor of scores\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Collect scores tensors\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ccl/Code/nerf-qa/play.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m all_scores\u001b[39m.\u001b[39mappend(scores)\n",
      "File \u001b[0;32m~/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Code/nerf-qa/nerf_qa/DISTS_pytorch/DISTS_pt.py:102\u001b[0m, in \u001b[0;36mDISTS.forward\u001b[0;34m(self, x, y, require_grad, batch_average)\u001b[0m\n\u001b[1;32m     99\u001b[0m S1 \u001b[39m=\u001b[39m (\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mx_mean\u001b[39m*\u001b[39my_mean\u001b[39m+\u001b[39mc1)\u001b[39m/\u001b[39m(x_mean\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m+\u001b[39my_mean\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m+\u001b[39mc1)\n\u001b[1;32m    100\u001b[0m dist1 \u001b[39m=\u001b[39m dist1\u001b[39m+\u001b[39m(alpha[k]\u001b[39m*\u001b[39mS1)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m,keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 102\u001b[0m x_var \u001b[39m=\u001b[39m ((feats0[k]\u001b[39m-\u001b[39;49mx_mean)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean([\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m], keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    103\u001b[0m y_var \u001b[39m=\u001b[39m ((feats1[k]\u001b[39m-\u001b[39my_mean)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean([\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m], keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    104\u001b[0m xy_cov \u001b[39m=\u001b[39m (feats0[k]\u001b[39m*\u001b[39mfeats1[k])\u001b[39m.\u001b[39mmean([\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m],keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m x_mean\u001b[39m*\u001b[39my_mean\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.78 GiB. GPU 0 has a total capacity of 11.76 GiB of which 918.19 MiB is free. Including non-PyTorch memory, this process has 10.47 GiB memory in use. Of the allocated memory 7.01 GiB is allocated by PyTorch, and 2.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "scores_df = pd.read_csv(SCORE_FILE)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Initialize a new run with wandb\n",
    "wandb.init(project='nerf-qa', config={\n",
    "    \"seed\": 42,\n",
    "    \"resize\": True,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 1,\n",
    "    \"forward_batch_size\": 32, # only affects training time and memory usage\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": 1e-4,\n",
    "        \"eps\": 1e-8,\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999,\n",
    "    },\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# Number of splits for GroupKFold\n",
    "num_folds = min(scores_df['reference_filename'].nunique(), 4)\n",
    "\n",
    "# Example function to load a video and process it frame by frame\n",
    "def load_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert frame to RGB (from BGR) and then to tensor\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "        frame = transforms.ToPILImage()(frame)\n",
    "        frame = prepare_image(frame, resize=config.resize).squeeze(0)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return torch.stack(frames)\n",
    "\n",
    "# Batch creation function\n",
    "def create_dataloader(row, forward_batch_size):\n",
    "    dist_video_path = path.join(SYN_DIR, row['distorted_filename'])\n",
    "    ref_video_path = path.join(REF_DIR, row['reference_filename'])\n",
    "    ref = load_video_frames(ref_video_path)\n",
    "    dist = load_video_frames(dist_video_path)\n",
    "    # Create a dataset and dataloader for efficient batching\n",
    "    dataset = TensorDataset(dist, ref)\n",
    "    dataloader = DataLoader(dataset, batch_size=forward_batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "# Initialize GroupKFold\n",
    "gkf = GroupKFold(n_splits=num_folds)\n",
    "\n",
    "# Extract reference filenames as groups for GroupKFold\n",
    "groups = scores_df['reference_filename'].values\n",
    "\n",
    "global_step = 0\n",
    "plccs = []\n",
    "srccs = []\n",
    "rsmes = []\n",
    "\n",
    "# Group K-Fold Cross-Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(scores_df, groups=groups), 1):\n",
    "    print(f\"Fold {fold}/{num_folds}\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    train_df = scores_df.iloc[train_idx]\n",
    "    val_df = scores_df.iloc[val_idx]\n",
    "    train_size = train_df.shape[0]\n",
    "    val_size = val_df.shape[0]\n",
    "\n",
    "    print(f\"Validation Refrences: {val_df['reference_filename'].drop_duplicates().values}\")\n",
    "\n",
    "    # Reset model and optimizer for each fold (if you want to start fresh for each fold)\n",
    "    model = VQAModel(train_df=train_df).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "        lr=config.optimizer['lr'],\n",
    "        betas=(config.optimizer['beta1'], config.optimizer['beta2']),\n",
    "        eps=config.optimizer['eps']\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(wandb.config.epochs):\n",
    "        print(f\"Epoch {epoch+1}/{wandb.config.epochs}\")\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        batch_loss = 0\n",
    "        optimizer.zero_grad()  # Initialize gradients to zero at the start of each epoch\n",
    "\n",
    "        # Shuffle train_df with random seed\n",
    "        train_df = train_df.sample(frac=1, random_state=config.seed+global_step).reset_index(drop=True)\n",
    "        for index, (i, row) in tqdm(enumerate(train_df.iterrows(), 1), total=train_size, desc=\"Training...\"):  # Start index from 1 for easier modulus operation\n",
    "            # Load frames\n",
    "            dataloader = create_dataloader(row, config.forward_batch_size)\n",
    "            \n",
    "            # Compute score\n",
    "            predicted_score = model(dataloader)\n",
    "            target_score = torch.tensor(row['MOS'], device=device, dtype=torch.float32)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(predicted_score, target_score)\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "            if index % config.batch_size == 0 or index == train_size:\n",
    "\n",
    "                # Scale gradients\n",
    "                accumulation_steps = ((index-1) % config.batch_size) + 1\n",
    "                global_step += accumulation_steps\n",
    "                if accumulation_steps > 1:\n",
    "                    for param in model.parameters():\n",
    "                        if param.grad is not None:\n",
    "                            param.grad /= accumulation_steps\n",
    "                \n",
    "                # Update parameters every batch_size steps or on the last iteration\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()  # Zero the gradients after updating\n",
    "                average_batch_loss = batch_loss / config.batch_size\n",
    "                wandb.log({\n",
    "                    \"Train Metrics Dict/batch_loss\": average_batch_loss,\n",
    "                    \"Train Metrics Dict/rmse\": np.sqrt(average_batch_loss),\n",
    "                    }, step=global_step)\n",
    "                batch_loss = 0\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            eval_loss = 0\n",
    "            all_rmse = []\n",
    "            all_target_scores = []  # List to store all target scores\n",
    "            all_predicted_scores = []  # List to store all predicted scores\n",
    "\n",
    "            for index, row in tqdm(val_df.iterrows(), total=val_size, desc=\"Validating...\"):\n",
    "                # Load frames\n",
    "                dataloader = create_dataloader(row, config.forward_batch_size)\n",
    "                \n",
    "                # Compute score\n",
    "                predicted_score = model(dataloader)\n",
    "                target_score = torch.tensor(row['MOS'], device=device, dtype=torch.float32)\n",
    "                all_predicted_scores.append(float(predicted_score.item()))\n",
    "                all_target_scores.append(float(target_score.item()))\n",
    "            \n",
    "                # Compute loss\n",
    "                loss = loss_fn(predicted_score, target_score)\n",
    "                eval_loss += loss.item()\n",
    "                all_rmse.append(float(np.sqrt(loss.item())))\n",
    "\n",
    "            \n",
    "            # Convert lists to arrays for correlation computation\n",
    "            all_target_scores = np.array(all_target_scores)\n",
    "            all_predicted_scores = np.array(all_predicted_scores)\n",
    "            \n",
    "            # Compute PLCC and SRCC\n",
    "            plcc = pearsonr(all_target_scores, all_predicted_scores)[0]\n",
    "            srcc = spearmanr(all_target_scores, all_predicted_scores)[0]\n",
    "            \n",
    "            # Average loss over validation set\n",
    "            eval_loss /= len(val_df)\n",
    "            rsme = np.mean(all_rmse)\n",
    "\n",
    "            if epoch == wandb.config.epochs-1:\n",
    "                # last epoch\n",
    "                plccs.append(float(plcc))\n",
    "                srccs.append(float(srcc))\n",
    "                rsmes.append(float(rsme))\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                \"Eval Metrics Dict/batch_loss\": eval_loss,\n",
    "                \"Eval Metrics Dict/rmse\": rsme,\n",
    "                \"Eval Metrics Dict/plcc\": plcc,\n",
    "                \"Eval Metrics Dict/srcc\": srcc,\n",
    "            }, step=global_step)\n",
    "            wandb.log({\n",
    "                \"Eval Metrics Dict/rmse_hist\": wandb.Histogram(np.array(all_rmse)),\n",
    "            }, step=global_step)\n",
    "\n",
    "            \n",
    "        # Logging the average loss\n",
    "        average_loss = total_loss / len(scores_df)\n",
    "        print(f\"Average Loss: {average_loss}\\n\\n\")\n",
    "        wandb.log({ \"Train Metrics Dict/total_loss\": average_batch_loss }, step=global_step)\n",
    "\n",
    "weighted_score = -1.0 * np.mean(rsmes) + 1.0 * np.mean(plccs) + 1.0 * np.mean(srccs)\n",
    "# Log to wandb\n",
    "wandb.log({\n",
    "    \"Cross-Validation Metrics Dict/weighted_score_mean\": weighted_score,\n",
    "    \"Cross-Validation Metrics Dict/rmse_mean\": np.mean(rsmes),\n",
    "    \"Cross-Validation Metrics Dict/rmse_std\": np.std(rsmes),\n",
    "    \"Cross-Validation Metrics Dict/plcc_mean\": np.mean(plccs),\n",
    "    \"Cross-Validation Metrics Dict/plcc_std\": np.std(plccs),\n",
    "    \"Cross-Validation Metrics Dict/srcc_mean\": np.mean(srccs),\n",
    "    \"Cross-Validation Metrics Dict/srcc_std\": np.std(srccs),\n",
    "}, step=global_step)\n",
    "wandb.log({\n",
    "    \"Cross-Validation Metrics Dict/rmse_hist\": wandb.Histogram(np.array(rsmes)),\n",
    "    \"Cross-Validation Metrics Dict/plcc_hist\": wandb.Histogram(np.array(plccs)),\n",
    "    \"Cross-Validation Metrics Dict/srcc_hist\": wandb.Histogram(np.array(srccs)),\n",
    "}, step=global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
