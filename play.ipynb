{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58988/1504963849.py:18: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# system level\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "\n",
    "# deep learning\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models,transforms\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# data \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# local\n",
    "from nerf_qa.DISTS_pytorch.DISTS_pt import DISTS, prepare_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/ccl/Datasets/NeRF-QA\"\n",
    "REF_DIR = path.join(DATA_DIR, \"Reference\")\n",
    "SYN_DIR = path.join(DATA_DIR, \"NeRF-QA_videos\")\n",
    "SCORE_FILE = path.join(DATA_DIR, \"NeRF_VQA_MOS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, dists_mean, dists_std):\n",
    "        super(VQAModel, self).__init__()\n",
    "        self.dists_model = DISTS()\n",
    "        # Initialize mean and std as trainable parameters\n",
    "        self.dists_mean = nn.Parameter(torch.tensor([dists_mean], dtype=torch.float32))\n",
    "        self.dists_std = nn.Parameter(torch.tensor([-dists_std], dtype=torch.float32))\n",
    "\n",
    "    def compute_dists_with_batches(self, ref_batches, dist_batches):\n",
    "        all_scores = []  # Collect scores from all batches as tensors\n",
    "\n",
    "        for ref_batch, dist_batch in zip(ref_batches, dist_batches):\n",
    "            ref_images = ref_batch[0].to(device)  # Assuming ref_batch[0] is the tensor of images\n",
    "            dist_images = dist_batch[0].to(device)  # Assuming dist_batch[0] is the tensor of images\n",
    "            scores = self.dists_model(ref_images, dist_images, require_grad=True, batch_average=False)  # Returns a tensor of scores\n",
    "            \n",
    "            # Collect scores tensors\n",
    "            all_scores.append(scores)\n",
    "\n",
    "        # Concatenate all score tensors into a single tensor\n",
    "        all_scores_tensor = torch.cat(all_scores, dim=0)\n",
    "\n",
    "        # Compute the average score across all batches\n",
    "        average_score = torch.mean(all_scores_tensor) if all_scores_tensor.numel() > 0 else torch.tensor(0.0).to(device)\n",
    "\n",
    "        return average_score\n",
    "        \n",
    "    def forward(self, ref_batches, dist_batches):\n",
    "        raw_scores = self.compute_dists_with_batches(ref_batches, dist_batches)\n",
    "        \n",
    "        # Normalize raw scores using the trainable mean and std\n",
    "        normalized_scores = (raw_scores - self.dists_mean) / self.dists_std\n",
    "        return normalized_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkobejean\u001b[0m (\u001b[33maizu-nerf\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ccl/Code/nerf-qa/wandb/run-20240216_211252-hzyxrfr9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aizu-nerf/nerf-qa/runs/hzyxrfr9' target=\"_blank\">glittering-cake-8</a></strong> to <a href='https://wandb.ai/aizu-nerf/nerf-qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aizu-nerf/nerf-qa' target=\"_blank\">https://wandb.ai/aizu-nerf/nerf-qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aizu-nerf/nerf-qa/runs/hzyxrfr9' target=\"_blank\">https://wandb.ai/aizu-nerf/nerf-qa/runs/hzyxrfr9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/8\n",
      "Fold Stats: MOS (2.9460675, 1.0108160114069558) DISTS (0.1080630695174138, 0.05652211786027942)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized scores: -0.6653708219528198 -0.09004860371351242\n",
      "normalized scores: 0.5071471929550171 0.08984322845935822\n",
      "normalized scores: 0.8732870221138 0.21695443987846375\n",
      "normalized scores: 0.52683424949646 0.03394448012113571\n",
      "accumulation_steps 4\n",
      "normalized scores: 1.387524962425232 1.2805109024047852\n",
      "normalized scores: 0.4404684007167816 1.2425769567489624\n",
      "normalized scores: -1.353824496269226 -0.4892864525318146\n",
      "normalized scores: 0.9242359399795532 0.6799938678741455\n",
      "accumulation_steps 4\n",
      "normalized scores: -1.558510661125183 -1.1971653699874878\n",
      "normalized scores: 0.877738893032074 1.0135070085525513\n",
      "normalized scores: 0.37487781047821045 0.747513473033905\n",
      "normalized scores: 1.4264044761657715 1.4860509634017944\n",
      "accumulation_steps 4\n",
      "normalized scores: -1.4700672626495361 -0.2760081887245178\n",
      "normalized scores: -0.8767842054367065 -0.4428960382938385\n",
      "normalized scores: -0.5009492039680481 -0.844426155090332\n",
      "normalized scores: 0.6589057445526123 1.2616506814956665\n",
      "accumulation_steps 4\n",
      "normalized scores: 0.3492549657821655 0.34461405873298645\n",
      "normalized scores: 0.7720816731452942 0.21788105368614197\n",
      "normalized scores: 1.6754112243652344 1.2409610748291016\n",
      "normalized scores: -0.5892937183380127 -1.2819207906723022\n",
      "accumulation_steps 4\n",
      "normalized scores: 1.243977665901184 1.1484447717666626\n",
      "normalized scores: 0.2616029977798462 0.768868088722229\n",
      "normalized scores: 1.6225826740264893 1.3951163291931152\n",
      "normalized scores: 0.292172372341156 -1.555504322052002\n",
      "accumulation_steps 4\n",
      "normalized scores: -1.1557667255401611 -0.7391237020492554\n",
      "normalized scores: 1.4259098768234253 1.1709187030792236\n",
      "normalized scores: 0.095598503947258 -0.5615242719650269\n",
      "normalized scores: -0.9359443187713623 -2.7292404174804688\n",
      "accumulation_steps 4\n",
      "normalized scores: 0.7157905101776123 1.3580188751220703\n",
      "normalized scores: -0.9781874418258667 -1.2188777923583984\n",
      "normalized scores: -1.0200347900390625 -0.876814067363739\n",
      "normalized scores: -0.4877915382385254 -0.897919237613678\n",
      "accumulation_steps 4\n",
      "normalized scores: -1.3958697319030762 -0.5840907096862793\n",
      "normalized scores: -1.2238305807113647 -1.3654781579971313\n",
      "normalized scores: -1.696913719177246 -0.5274698138237\n",
      "normalized scores: -0.6484538316726685 0.3451620638370514\n",
      "accumulation_steps 4\n",
      "normalized scores: 0.3859579861164093 1.4048011302947998\n",
      "normalized scores: 0.47855642437934875 0.47958365082740784\n",
      "normalized scores: 0.2702099084854126 -0.5732226371765137\n",
      "normalized scores: -1.0289385318756104 -1.3501030206680298\n",
      "accumulation_steps 4\n",
      "Epoch 1, Average Loss: 0.3993157335455682\n",
      "normalized scores: -0.6653708219528198 -0.06957856565713882\n",
      "normalized scores: 0.5071471929550171 0.21205247938632965\n",
      "normalized scores: 0.8732870221138 0.34088000655174255\n",
      "normalized scores: 0.52683424949646 0.7387475967407227\n",
      "accumulation_steps 4\n",
      "normalized scores: 1.387524962425232 0.8536574244499207\n",
      "normalized scores: 0.4404684007167816 0.7370420098304749\n",
      "normalized scores: -1.353824496269226 -1.3824872970581055\n",
      "normalized scores: 0.9242359399795532 0.24112696945667267\n",
      "accumulation_steps 4\n",
      "normalized scores: -1.558510661125183 -1.0409244298934937\n",
      "normalized scores: 0.877738893032074 0.6681662201881409\n",
      "normalized scores: 0.37487781047821045 0.9351983666419983\n",
      "normalized scores: 1.4264044761657715 1.364507794380188\n",
      "accumulation_steps 4\n",
      "normalized scores: -1.4700672626495361 -0.3666507303714752\n",
      "normalized scores: -0.8767842054367065 0.3472796380519867\n",
      "normalized scores: -0.5009492039680481 0.25439977645874023\n",
      "normalized scores: 0.6589057445526123 1.1338889598846436\n",
      "accumulation_steps 4\n",
      "normalized scores: 0.3492549657821655 0.4105922281742096\n",
      "normalized scores: 0.7720816731452942 1.1417254209518433\n",
      "normalized scores: 1.6754112243652344 1.2800451517105103\n",
      "normalized scores: -0.5892937183380127 -0.02701205015182495\n",
      "accumulation_steps 4\n",
      "normalized scores: 1.243977665901184 1.1285579204559326\n",
      "normalized scores: 0.2616029977798462 1.1574970483779907\n",
      "normalized scores: 1.6225826740264893 1.4111944437026978\n",
      "normalized scores: 0.292172372341156 0.7191476821899414\n",
      "accumulation_steps 4\n",
      "normalized scores: -1.1557667255401611 -0.4561881721019745\n",
      "normalized scores: 1.4259098768234253 1.174529790878296\n",
      "normalized scores: 0.095598503947258 0.4420994818210602\n",
      "normalized scores: -0.9359443187713623 -1.2055277824401855\n",
      "accumulation_steps 4\n",
      "normalized scores: 0.7157905101776123 1.297370195388794\n",
      "normalized scores: -0.9781874418258667 -0.6334741115570068\n",
      "normalized scores: -1.0200347900390625 -1.2413723468780518\n",
      "normalized scores: -0.4877915382385254 -0.23436696827411652\n",
      "accumulation_steps 4\n",
      "normalized scores: -1.3958697319030762 -1.09825599193573\n",
      "normalized scores: -1.2238305807113647 -1.9253509044647217\n",
      "normalized scores: -1.696913719177246 -1.0732629299163818\n",
      "normalized scores: -0.6484538316726685 -0.026556644588708878\n",
      "accumulation_steps 4\n",
      "normalized scores: 0.3859579861164093 1.2512091398239136\n",
      "normalized scores: 0.47855642437934875 -0.0041605159640312195\n",
      "normalized scores: 0.2702099084854126 -0.615898609161377\n",
      "normalized scores: -1.0289385318756104 -1.5999479293823242\n",
      "accumulation_steps 4\n",
      "Epoch 2, Average Loss: 0.25360874169443076\n",
      "normalized scores: -0.6653708219528198 -0.7374055981636047\n",
      "normalized scores: 0.5071471929550171 -0.3982541859149933\n",
      "normalized scores: 0.8732870221138 -0.24275456368923187\n",
      "normalized scores: 0.52683424949646 0.19657786190509796\n",
      "accumulation_steps 4\n",
      "normalized scores: 1.387524962425232 0.5985423922538757\n",
      "normalized scores: 0.4404684007167816 0.4385908544063568\n",
      "normalized scores: -1.353824496269226 -2.1947591304779053\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "scores_df = pd.read_csv(SCORE_FILE)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Initialize a new run with wandb\n",
    "wandb.init(project='nerf-qa', config={\n",
    "    \"seed\": 42,\n",
    "    \"resize\": True,\n",
    "    \"epochs\": 4,\n",
    "    \"batch_size\": 4,\n",
    "    \"forward_batch_size\": 64,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": 1e-4,\n",
    "        \"eps\": 1e-8,\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999,\n",
    "    },\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# Number of splits for GroupKFold\n",
    "unique_references_count = scores_df['reference_filename'].nunique()\n",
    "num_folds = min(unique_references_count, 8)\n",
    "\n",
    "# Example function to load a video and process it frame by frame\n",
    "def load_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert frame to RGB (from BGR) and then to tensor\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "        frame = transforms.ToPILImage()(frame)\n",
    "        frame = prepare_image(frame, resize=config.resize).squeeze(0)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return torch.stack(frames)\n",
    "\n",
    "# Batch creation function\n",
    "def create_batches(frames, forward_batch_size):\n",
    "    # Create a dataset and dataloader for efficient batching\n",
    "    dataset = TensorDataset(frames)\n",
    "    dataloader = DataLoader(dataset, batch_size=forward_batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Convert reference_filename to categorical codes for stratification\n",
    "# scores_df['ref_code'] = scores_df['reference_filename'].astype('category').cat.codes\n",
    "\n",
    "\n",
    "# Initialize GroupKFold\n",
    "gkf = GroupKFold(n_splits=num_folds)\n",
    "\n",
    "# Extract reference filenames as groups for GroupKFold\n",
    "groups = scores_df['reference_filename'].values\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "# Group K-Fold Cross-Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(scores_df, groups=groups), 1):\n",
    "    print(f\"Fold {fold}/{num_folds}\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    train_df = scores_df.iloc[train_idx]\n",
    "    val_df = scores_df.iloc[val_idx]\n",
    "\n",
    "    # Shuffle train_df with random seed\n",
    "    train_df = train_df.sample(frac=1, random_state=config.seed).reset_index(drop=True)\n",
    "\n",
    "    # Compute the mean and standard deviation for MOS and DISTS columns\n",
    "    mos_mean = train_df['MOS'].mean()\n",
    "    mos_std = train_df['MOS'].std()\n",
    "    dists_mean = train_df['DISTS'].mean()\n",
    "    dists_std = train_df['DISTS'].std()\n",
    "    print(f\"Fold Stats: MOS ({mos_mean}, {mos_std}) DISTS ({dists_mean}, {dists_std})\")\n",
    "    \n",
    "    # Reset model and optimizer for each fold (if you want to start fresh for each fold)\n",
    "    model = VQAModel(dists_mean=dists_mean, dists_std=dists_std).to(device)\n",
    "    betas = (config.optimizer['beta1'], config.optimizer['beta2'])\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.optimizer['lr'], betas=betas, eps=config.optimizer['eps'])\n",
    "    \n",
    "    steps_per_epoch = train_df.shape[0]\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(wandb.config.epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        batch_loss = 0\n",
    "        optimizer.zero_grad()  # Initialize gradients to zero at the start of each epoch\n",
    "\n",
    "        for index, (i, row) in enumerate(train_df.iterrows(), 1):  # Start index from 1 for easier modulus operation\n",
    "            # Load frames\n",
    "            dist_video_path = path.join(SYN_DIR, row['distorted_filename'])\n",
    "            ref_video_path = path.join(REF_DIR, row['reference_filename'])\n",
    "            ref = load_video_frames(ref_video_path)\n",
    "            dist = load_video_frames(dist_video_path)\n",
    "            ref = create_batches(ref, config.forward_batch_size)\n",
    "            dist = create_batches(dist, config.forward_batch_size)\n",
    "            \n",
    "            # Compute score\n",
    "            predicted_score = model(ref, dist)\n",
    "            \n",
    "            # Normalize scores\n",
    "            target_score_normalized = torch.tensor((row['MOS'] - mos_mean) / mos_std, device=device, dtype=torch.float32)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(predicted_score, target_score_normalized)\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            batch_loss += loss.item()\n",
    "            print(\"normalized scores:\", target_score_normalized.item(), predicted_score.item())\n",
    "            \n",
    "            if index % config.batch_size == 0 or index == steps_per_epoch:\n",
    "\n",
    "                # Scale gradients\n",
    "                accumulation_steps = ((index-1) % config.batch_size) + 1\n",
    "                print(\"accumulation_steps\", accumulation_steps)\n",
    "                global_step += accumulation_steps\n",
    "                for param in model.parameters():\n",
    "                    if param.grad is not None:\n",
    "                        param.grad /= accumulation_steps\n",
    "                \n",
    "                # Update parameters every batch_size steps or on the last iteration\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()  # Zero the gradients after updating\n",
    "                average_batch_loss = batch_loss / config.batch_size\n",
    "                wandb.log({ \"Train Metrics Dict/batch_loss\": average_batch_loss }, step=global_step)\n",
    "                batch_loss = 0\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            pass\n",
    "            \n",
    "\n",
    "\n",
    "        global_step = (epoch+1) * train_df.shape[0]\n",
    "        # Logging the average loss\n",
    "        average_loss = total_loss / len(scores_df)\n",
    "        print(f\"Epoch {epoch+1}, Average Loss: {average_loss}\")\n",
    "        wandb.log({ \"Train Metrics Dict/total_loss\": average_batch_loss }, step=global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique reference filenames: 8\n"
     ]
    }
   ],
   "source": [
    "# Count unique reference filenames\n",
    "unique_references_count = scores_df['reference_filename'].nunique()\n",
    "\n",
    "# Print the count\n",
    "print(f\"Number of unique reference filenames: {unique_references_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
