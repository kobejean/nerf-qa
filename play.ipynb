{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_336573/4239856797.py:20: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "# system level\n",
    "import os\n",
    "from os import path\n",
    "import sys\n",
    "\n",
    "\n",
    "# deep learning\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import models,transforms\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# data \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# local\n",
    "from nerf_qa.DISTS_pytorch.DISTS_pt import DISTS, prepare_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/home/ccl/Datasets/NeRF-QA\"\n",
    "REF_DIR = path.join(DATA_DIR, \"Reference\")\n",
    "SYN_DIR = path.join(DATA_DIR, \"NeRF-QA_videos\")\n",
    "SCORE_FILE = path.join(DATA_DIR, \"NeRF_VQA_MOS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, train_df):\n",
    "        super(VQAModel, self).__init__()\n",
    "        # Reshape data (scikit-learn expects X to be a 2D array)\n",
    "        X = train_df['DISTS'].values.reshape(-1, 1)  # Predictor\n",
    "        y = train_df['MOS'].values  # Response\n",
    "\n",
    "        # Create a linear regression model\n",
    "        model = LinearRegression()\n",
    "\n",
    "        # Fit the model\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Print the coefficients\n",
    "        print(f\"Coefficient: {model.coef_[0]}\")\n",
    "        print(f\"Intercept: {model.intercept_}\")\n",
    "        self.dists_model = DISTS()\n",
    "        self.dists_weight = nn.Parameter(torch.tensor([model.coef_[0]], dtype=torch.float32))\n",
    "        self.dists_bias = nn.Parameter(torch.tensor([model.intercept_], dtype=torch.float32))\n",
    "\n",
    "    def compute_dists_with_batches(self, dataloader):\n",
    "        all_scores = []  # Collect scores from all batches as tensors\n",
    "\n",
    "        for dist_batch, ref_batch in dataloader:\n",
    "            ref_images = ref_batch.to(device)  # Assuming ref_batch[0] is the tensor of images\n",
    "            dist_images = dist_batch.to(device)  # Assuming dist_batch[0] is the tensor of images\n",
    "            scores = self.dists_model(ref_images, dist_images, require_grad=True, batch_average=False)  # Returns a tensor of scores\n",
    "            \n",
    "            # Collect scores tensors\n",
    "            all_scores.append(scores)\n",
    "\n",
    "        # Concatenate all score tensors into a single tensor\n",
    "        all_scores_tensor = torch.cat(all_scores, dim=0)\n",
    "\n",
    "        # Compute the average score across all batches\n",
    "        average_score = torch.mean(all_scores_tensor) if all_scores_tensor.numel() > 0 else torch.tensor(0.0).to(device)\n",
    "\n",
    "        return average_score\n",
    "        \n",
    "    def forward(self, dataloader):\n",
    "        raw_scores = self.compute_dists_with_batches(dataloader)\n",
    "        \n",
    "        # Normalize raw scores using the trainable mean and std\n",
    "        normalized_scores = raw_scores * self.dists_weight + self.dists_bias\n",
    "        return normalized_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkobejean\u001b[0m (\u001b[33maizu-nerf\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ccl/Code/nerf-qa/wandb/run-20240217_104543-ja2xlg56</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aizu-nerf/nerf-qa/runs/ja2xlg56' target=\"_blank\">enchanting-dragon-24</a></strong> to <a href='https://wandb.ai/aizu-nerf/nerf-qa' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aizu-nerf/nerf-qa' target=\"_blank\">https://wandb.ai/aizu-nerf/nerf-qa</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aizu-nerf/nerf-qa/runs/ja2xlg56' target=\"_blank\">https://wandb.ai/aizu-nerf/nerf-qa/runs/ja2xlg56</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/4\n",
      "Validation Refrences: ['ship_reference.mp4' 'truck_reference.mp4']\n",
      "Coefficient: -14.04897111263456\n",
      "Intercept: 4.465621463615414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/36 [00:00<?, ?it/s]/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training...: 100%|██████████| 36/36 [02:47<00:00,  4.66s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:58<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.3772285995740579\n",
      "\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:47<00:00,  4.66s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:57<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.17917628636253843\n",
      "\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:47<00:00,  4.65s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:58<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.17956307262162832\n",
      "\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:46<00:00,  4.63s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:57<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.1749747391489412\n",
      "\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:45<00:00,  4.60s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:55<00:00,  4.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.16986569139771746\n",
      "\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:36<00:00,  4.34s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:57<00:00,  4.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.15266524353099462\n",
      "\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:34<00:00,  4.30s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.13948929940670496\n",
      "\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.13s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.1176052463955178\n",
      "\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.12s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0839223544341318\n",
      "\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.12701219658144206\n",
      "\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.12s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.10101679860167678\n",
      "\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.13s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.079196500321198\n",
      "\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.09s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.05929466691723917\n",
      "\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0634772892305288\n",
      "\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.10s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.05874466221606175\n",
      "\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.12s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.05284183756369506\n",
      "\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.06611718979578048\n",
      "\n",
      "\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.055370332831690426\n",
      "\n",
      "\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.12s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.05813445668794278\n",
      "\n",
      "\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.052125223765225805\n",
      "\n",
      "\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.10s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.05920100952304589\n",
      "\n",
      "\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.055622798707796996\n",
      "\n",
      "\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:38<00:00,  4.41s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:57<00:00,  4.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0507955841032981\n",
      "\n",
      "\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:46<00:00,  4.61s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:57<00:00,  4.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0855032382682263\n",
      "\n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:45<00:00,  4.60s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:57<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.05652947628914262\n",
      "\n",
      "\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:45<00:00,  4.60s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:57<00:00,  4.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.05703213893550204\n",
      "\n",
      "\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:39<00:00,  4.43s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.03793379508624639\n",
      "\n",
      "\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.10s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.043400975248687246\n",
      "\n",
      "\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.10s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.037223255123838804\n",
      "\n",
      "\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.04365473833222211\n",
      "\n",
      "\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.10s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.04342876419658145\n",
      "\n",
      "\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.05408453199076272\n",
      "\n",
      "\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.09s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0439758166065379\n",
      "\n",
      "\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.09s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.04103471441339934\n",
      "\n",
      "\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.10s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.04332877120396006\n",
      "\n",
      "\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.10s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.04520819568430549\n",
      "\n",
      "\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.03601860874656874\n",
      "\n",
      "\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0396146706286243\n",
      "\n",
      "\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:27<00:00,  4.11s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:54<00:00,  4.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0341103789172242\n",
      "\n",
      "\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:46<00:00,  4.62s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:56<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.03494379348421717\n",
      "\n",
      "\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:40<00:00,  4.47s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:52<00:00,  4.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.04061154052639419\n",
      "\n",
      "\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:30<00:00,  4.17s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:52<00:00,  4.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.07837668625188599\n",
      "\n",
      "\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:31<00:00,  4.20s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:57<00:00,  4.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.12138573865922808\n",
      "\n",
      "\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:45<00:00,  4.61s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:54<00:00,  4.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.07546969237440256\n",
      "\n",
      "\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:37<00:00,  4.36s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:55<00:00,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.05489072908059711\n",
      "\n",
      "\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:32<00:00,  4.24s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:52<00:00,  4.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.046722320317655885\n",
      "\n",
      "\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:31<00:00,  4.20s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:52<00:00,  4.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.0333186607740951\n",
      "\n",
      "\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:35<00:00,  4.32s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.03277991065897368\n",
      "\n",
      "\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:34<00:00,  4.28s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.031646262597860186\n",
      "\n",
      "\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:31<00:00,  4.20s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.27s/it]\n",
      "/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.034490843099149515\n",
      "\n",
      "\n",
      "Fold 2/4\n",
      "Validation Refrences: ['lego_reference.mp4' 'train_reference.mp4']\n",
      "Coefficient: -15.752244268703977\n",
      "Intercept: 4.5291358024327755\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:   0%|          | 0/36 [00:00<?, ?it/s]/home/ccl/anaconda3/envs/nerf-qa/lib/python3.10/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Training...: 100%|██████████| 36/36 [02:29<00:00,  4.15s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:50<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.31525405858459027\n",
      "\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.13s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.157763656304193\n",
      "\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:31<00:00,  4.21s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:53<00:00,  4.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.15965783496229355\n",
      "\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:33<00:00,  4.28s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.1387718272441513\n",
      "\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:28<00:00,  4.14s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.10284117687806926\n",
      "\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...: 100%|██████████| 36/36 [02:29<00:00,  4.16s/it]\n",
      "Validating...: 100%|██████████| 12/12 [00:51<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Loss: 0.09991360375473353\n",
      "\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training...:  11%|█         | 4/36 [00:22<02:57,  5.56s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m train_df \u001b[38;5;241m=\u001b[39m train_df\u001b[38;5;241m.\u001b[39msample(frac\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed\u001b[38;5;241m+\u001b[39mglobal_step)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, (i, row) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_df\u001b[38;5;241m.\u001b[39miterrows(), \u001b[38;5;241m1\u001b[39m), total\u001b[38;5;241m=\u001b[39mtrain_size, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Start index from 1 for easier modulus operation\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;66;03m# Load frames\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_batch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# Compute score\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     predicted_score \u001b[38;5;241m=\u001b[39m model(dataloader)\n",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m, in \u001b[0;36mcreate_dataloader\u001b[0;34m(row, forward_batch_size)\u001b[0m\n\u001b[1;32m     47\u001b[0m ref_video_path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mjoin(REF_DIR, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreference_filename\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     48\u001b[0m ref \u001b[38;5;241m=\u001b[39m load_video_frames(ref_video_path)\n\u001b[0;32m---> 49\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[43mload_video_frames\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdist_video_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Create a dataset and dataloader for efficient batching\u001b[39;00m\n\u001b[1;32m     51\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TensorDataset(dist, ref)\n",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mload_video_frames\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m     frame \u001b[38;5;241m=\u001b[39m prepare_image(frame, resize\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mresize)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     40\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mcap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelease\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(frames)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read the CSV file\n",
    "scores_df = pd.read_csv(SCORE_FILE)\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "\n",
    "# Initialize a new run with wandb\n",
    "wandb.init(project='nerf-qa', config={\n",
    "    \"seed\": 42,\n",
    "    \"resize\": True,\n",
    "    \"epochs\": 50,\n",
    "    \"batch_size\": 1,\n",
    "    \"forward_batch_size\": 64, # only affects training time and memory usage\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"adam\",\n",
    "        \"lr\": 1e-4,\n",
    "        \"eps\": 1e-8,\n",
    "        \"beta1\": 0.9,\n",
    "        \"beta2\": 0.999,\n",
    "    },\n",
    "})\n",
    "config = wandb.config\n",
    "\n",
    "# Number of splits for GroupKFold\n",
    "num_folds = min(scores_df['reference_filename'].nunique(), 4)\n",
    "\n",
    "# Example function to load a video and process it frame by frame\n",
    "def load_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        # Convert frame to RGB (from BGR) and then to tensor\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = torch.from_numpy(frame).permute(2, 0, 1).float() / 255.0\n",
    "        frame = transforms.ToPILImage()(frame)\n",
    "        frame = prepare_image(frame, resize=config.resize).squeeze(0)\n",
    "        frames.append(frame)\n",
    "    cap.release()\n",
    "    return torch.stack(frames)\n",
    "\n",
    "# Batch creation function\n",
    "def create_dataloader(row, forward_batch_size):\n",
    "    dist_video_path = path.join(SYN_DIR, row['distorted_filename'])\n",
    "    ref_video_path = path.join(REF_DIR, row['reference_filename'])\n",
    "    ref = load_video_frames(ref_video_path)\n",
    "    dist = load_video_frames(dist_video_path)\n",
    "    # Create a dataset and dataloader for efficient batching\n",
    "    dataset = TensorDataset(dist, ref)\n",
    "    dataloader = DataLoader(dataset, batch_size=forward_batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "# Initialize GroupKFold\n",
    "gkf = GroupKFold(n_splits=num_folds)\n",
    "\n",
    "# Extract reference filenames as groups for GroupKFold\n",
    "groups = scores_df['reference_filename'].values\n",
    "\n",
    "global_step = 0\n",
    "plccs = []\n",
    "srccs = []\n",
    "rsmes = []\n",
    "\n",
    "# Group K-Fold Cross-Validation\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(scores_df, groups=groups), 1):\n",
    "    print(f\"Fold {fold}/{num_folds}\")\n",
    "    \n",
    "    # Split the data into training and validation sets\n",
    "    train_df = scores_df.iloc[train_idx]\n",
    "    val_df = scores_df.iloc[val_idx]\n",
    "    train_size = train_df.shape[0]\n",
    "    val_size = val_df.shape[0]\n",
    "\n",
    "    print(f\"Validation Refrences: {val_df['reference_filename'].drop_duplicates().values}\")\n",
    "\n",
    "    # Reset model and optimizer for each fold (if you want to start fresh for each fold)\n",
    "    model = VQAModel(train_df=train_df).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(),\n",
    "        lr=config.optimizer['lr'],\n",
    "        betas=(config.optimizer['beta1'], config.optimizer['beta2']),\n",
    "        eps=config.optimizer['eps']\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(wandb.config.epochs):\n",
    "        print(f\"Epoch {epoch+1}/{wandb.config.epochs}\")\n",
    "        model.train()  # Set model to training mode\n",
    "        total_loss = 0\n",
    "        batch_loss = 0\n",
    "        optimizer.zero_grad()  # Initialize gradients to zero at the start of each epoch\n",
    "\n",
    "        # Shuffle train_df with random seed\n",
    "        train_df = train_df.sample(frac=1, random_state=config.seed+global_step).reset_index(drop=True)\n",
    "        for index, (i, row) in tqdm(enumerate(train_df.iterrows(), 1), total=train_size, desc=\"Training...\"):  # Start index from 1 for easier modulus operation\n",
    "            # Load frames\n",
    "            dataloader = create_dataloader(row, config.forward_batch_size)\n",
    "            \n",
    "            # Compute score\n",
    "            predicted_score = model(dataloader)\n",
    "            target_score = torch.tensor(row['MOS'], device=device, dtype=torch.float32)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = loss_fn(predicted_score, target_score)\n",
    "            \n",
    "            # Accumulate gradients\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "            batch_loss += loss.item()\n",
    "            \n",
    "            if index % config.batch_size == 0 or index == train_size:\n",
    "\n",
    "                # Scale gradients\n",
    "                accumulation_steps = ((index-1) % config.batch_size) + 1\n",
    "                global_step += accumulation_steps\n",
    "                if accumulation_steps > 1:\n",
    "                    for param in model.parameters():\n",
    "                        if param.grad is not None:\n",
    "                            param.grad /= accumulation_steps\n",
    "                \n",
    "                # Update parameters every batch_size steps or on the last iteration\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()  # Zero the gradients after updating\n",
    "                average_batch_loss = batch_loss / config.batch_size\n",
    "                wandb.log({\n",
    "                    \"Train Metrics Dict/batch_loss\": average_batch_loss,\n",
    "                    \"Train Metrics Dict/rmse\": np.sqrt(average_batch_loss),\n",
    "                    }, step=global_step)\n",
    "                batch_loss = 0\n",
    "        \n",
    "        # Validation step\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            eval_loss = 0\n",
    "            all_rmse = []\n",
    "            all_target_scores = []  # List to store all target scores\n",
    "            all_predicted_scores = []  # List to store all predicted scores\n",
    "\n",
    "            for index, row in tqdm(val_df.iterrows(), total=val_size, desc=\"Validating...\"):\n",
    "                # Load frames\n",
    "                dataloader = create_dataloader(row, config.forward_batch_size)\n",
    "                \n",
    "                # Compute score\n",
    "                predicted_score = model(dataloader)\n",
    "                target_score = torch.tensor(row['MOS'], device=device, dtype=torch.float32)\n",
    "                all_predicted_scores.append(float(predicted_score.item()))\n",
    "                all_target_scores.append(float(target_score.item()))\n",
    "            \n",
    "                # Compute loss\n",
    "                loss = loss_fn(predicted_score, target_score)\n",
    "                eval_loss += loss.item()\n",
    "                all_rmse.append(float(np.sqrt(loss.item())))\n",
    "\n",
    "            \n",
    "            # Convert lists to arrays for correlation computation\n",
    "            all_target_scores = np.array(all_target_scores)\n",
    "            all_predicted_scores = np.array(all_predicted_scores)\n",
    "            \n",
    "            # Compute PLCC and SRCC\n",
    "            plcc = pearsonr(all_target_scores, all_predicted_scores)[0]\n",
    "            srcc = spearmanr(all_target_scores, all_predicted_scores)[0]\n",
    "            \n",
    "            # Average loss over validation set\n",
    "            eval_loss /= len(val_df)\n",
    "            rsme = np.mean(all_rmse)\n",
    "\n",
    "            if epoch == wandb.config.epochs-1:\n",
    "                # last epoch\n",
    "                plccs.append(float(plcc))\n",
    "                srccs.append(float(srcc))\n",
    "                rsmes.append(float(rsme))\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log({\n",
    "                \"Eval Metrics Dict/batch_loss\": eval_loss,\n",
    "                \"Eval Metrics Dict/rmse\": rsme,\n",
    "                \"Eval Metrics Dict/plcc\": plcc,\n",
    "                \"Eval Metrics Dict/srcc\": srcc,\n",
    "            }, step=global_step)\n",
    "            wandb.log({\n",
    "                \"Eval Metrics Dict/rmse_hist\": wandb.Histogram(np.array(all_rmse)),\n",
    "            }, step=global_step)\n",
    "\n",
    "            \n",
    "        # Logging the average loss\n",
    "        average_loss = total_loss / len(scores_df)\n",
    "        print(f\"Average Loss: {average_loss}\\n\\n\")\n",
    "        wandb.log({ \"Train Metrics Dict/total_loss\": average_batch_loss }, step=global_step)\n",
    "\n",
    "weighted_score = -1.0 * np.mean(rsmes) + 1.0 * np.mean(plccs) + 1.0 * np.mean(srccs)\n",
    "# Log to wandb\n",
    "wandb.log({\n",
    "    \"Cross-Validation Metrics Dict/weighted_score_mean\": weighted_score,\n",
    "    \"Cross-Validation Metrics Dict/rmse_mean\": np.mean(rsmes),\n",
    "    \"Cross-Validation Metrics Dict/rmse_std\": np.std(rsmes),\n",
    "    \"Cross-Validation Metrics Dict/plcc_mean\": np.mean(plccs),\n",
    "    \"Cross-Validation Metrics Dict/plcc_std\": np.std(plccs),\n",
    "    \"Cross-Validation Metrics Dict/srcc_mean\": np.mean(srccs),\n",
    "    \"Cross-Validation Metrics Dict/srcc_std\": np.std(srccs),\n",
    "}, step=global_step)\n",
    "wandb.log({\n",
    "    \"Cross-Validation Metrics Dict/rmse_hist\": wandb.Histogram(np.array(rsmes)),\n",
    "    \"Cross-Validation Metrics Dict/plcc_hist\": wandb.Histogram(np.array(plccs)),\n",
    "    \"Cross-Validation Metrics Dict/srcc_hist\": wandb.Histogram(np.array(srccs)),\n",
    "}, step=global_step)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf-qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
